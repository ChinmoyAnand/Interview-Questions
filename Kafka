1) What is Data Log in Kafka?

As we know, messages are retained for a considerable amount of time in Kafka. Moreover, there is flexibility for consumers that they can read as per their convenience. 
Although, there is a possible case that if Kafka is configured to keep messages for 24 hours and possibly that time consumer is down for time greater than 24 hours, 
then the consumer may lose those messages. However, still, we can read those messages from last known offset, 
but only at a condition that the downtime on part of the consumer is just 60 minutes. Moreover, on what consumers are reading from a topic Kafka doesnâ€™t keep state.


2) How to verify where the zookeeper stores information about Broker id and Offset numbers? (if He knows ask to write command).

Here is the command to check both metadata of offset and offsets. The offsets are stored in Zookeeper in the in _consumer_offsets topic. 
$ kafka-console-consumer.sh --consumer.config /tmp/consumer.config --formatter "GroupMetaDataManager\$offsetsMessageFormatter" --zookeeper : --topic _consumer_offsets 
The above command gives metadata of console consumer.

3) What ensures load balancing of the server in Kafka?

Ans. As the main role of the Leader is to perform the task of all read and write requests for the partition, 
whereas Followers passively replicate the leader. Hence, at the time of Leader failing, one of the Followers takeover the role of the Leader. 
Basically, this entire process ensures load balancing of the servers.

4) What are the major components of Kafka?

5. How do you handle message delivery guarantees in Kafka? Can you describe scenarios where you might choose different levels of guarantees?

Ideal Answer (5 star)
Kafka provides three levels of message delivery guarantees: at most once, at least once, and exactly once. The choice depends on the specific requirements of the application. For scenarios where data loss is acceptable, such as in real-time monitoring or logging, I might opt for at most once delivery to prioritize low latency. In contrast, for critical data that must not be lost or duplicated, such as financial transactions, I'd choose exactly once semantics, even though it might come with a performance overhead. At least once delivery provides a balance between reliability and performance and is suitable for many use cases where occasional duplicates can be handled gracefully.

6. magine you're tasked with implementing a real-time analytics system using Kafka. How would you design the Kafka topics and consumers to efficiently process and analyze incoming data streams?

Ideal Answer (5 star)
For a real-time analytics system, I would design the Kafka topics based on the types of data to be analyzed and the desired granularity of analysis. For instance, I might have separate topics for different types of events such as user actions, system metrics, or business transactions. Within each topic, I would partition data based on relevant keys or attributes to ensure parallel processing and scalability. Additionally, I would configure appropriate retention policies and compaction strategies to manage data retention and ensure efficient storage usage. As for consumers, I would implement specialized analytics consumers tailored to specific use cases or data types. These consumers would subscribe to the relevant topics and apply real-time processing and aggregation logic to derive insights from incoming data streams. To optimize performance, I would parallelize data processing using Kafka Streams or similar stream processing frameworks. Furthermore, I would leverage stateful processing to maintain contextual information and support complex analytics operations such as windowed aggregations or sessionization. Overall, the goal would be to design a scalable and flexible architecture capable of handling high-volume data streams while providing timely and accurate analytics results.

7. How do consumers consumes messages in Kafka?

The transfer of messages is done in Kafka by making use of send file API. The transfer of bytes occurs using this file through the kernel-space and the calls between back to the kernel and kernel user.

8. What role does ZooKeeper play in a cluster of Kafka?

Apache ZooKeeper acts as a distributed, open-source configuration and synchronization service, along with being a naming registry for distributed applications. It keeps track of the status of the Kafka cluster nodes, as well as of Kafka topics, partitions, etc. Since the data is divided across collections of nodes within ZooKeeper, it exhibits high availability and consistency. When a node fails, ZooKeeper performs an instant failover migration. ZooKeeper is used in Kafka for managing service discovery for Kafka brokers, which form the cluster. ZooKeeper communicates with Kafka when a new broker joins, when a broker dies, when a topic gets removed, or when a topic is added so that each node in the cluster knows about these changes. Thus, it provides an in-sync view of the Kafka cluster configuration.


9. Implementing Transactions in Kafka Streams
You are tasked with implementing a Kafka Streams application that requires transactional guarantees. How would you ensure that the processing and output of the stream are handled transactionally?
Ideal Answer (5 Star)
In Kafka Streams, to ensure transactional processing, I would enable exactly-once semantics by setting `processing.guarantee` to `exactly_once_v2` in the Streams configuration. This setting ensures that Kafka Streams commits both the input offsets and the output results atomically. Additionally, I would configure the source and sink topics with `enable.idempotence=true` and ensure that the transactional ID is set for the producer to maintain a consistent transactional context. Using Kafka's built-in support for transactions ensures that each message is processed exactly once, and results are applied only if the entire transactional unit succeeds.


10. 
